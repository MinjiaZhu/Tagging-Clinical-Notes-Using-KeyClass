{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgaNS0E8VWFr",
        "outputId": "5524905e-e032-4334-9d9d-e03276453674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/KeyClass_Private"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEJ3cn6sWqqH",
        "outputId": "5d226d65-5872-4df6-fb0f-b530ec8a0148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KeyClass_Private\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch\n",
        "!pip install cudatoolkit\n",
        "!pip install snorkel==0.9.8\n",
        "!pip install tokenizers==0.10.3\n",
        "!pip install transformers==4.11.3\n",
        "!pip install sentence-transformers==2.2.2\n",
        "#!pip install jupyter notebook\n",
        "#!pip install snorkel\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osi_utYuXKvC",
        "outputId": "88cebfb9-9913-44d3-e9f0-9214dd35c70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for cudatoolkit\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting snorkel==0.9.8\n",
            "  Downloading snorkel-0.9.8-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting munkres>=1.0.6 (from snorkel==0.9.8)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Collecting numpy<1.20.0,>=1.16.5 (from snorkel==0.9.8)\n",
            "  Downloading numpy-1.19.5.zip (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from snorkel==0.9.8) (1.11.4)\n",
            "Collecting pandas<2.0.0,>=1.0.0 (from snorkel==0.9.8)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from snorkel==0.9.8) (4.66.2)\n",
            "Collecting scikit-learn<0.25.0,>=0.20.2 (from snorkel==0.9.8)\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting tokenizers==0.10.3\n",
            "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers==4.11.3\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.11.3)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.11.3) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.11.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.11.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.11.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.11.3) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.11.3) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.11.3) (1.4.0)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=429757b345c47379d0011843de4696c1a3a9010b1d38515570ed99364fa3805b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2\n",
            "Collecting snorkel\n",
            "  Downloading snorkel-0.9.9-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting munkres>=1.0.6 (from snorkel)\n",
            "  Using cached munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from snorkel) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from snorkel) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from snorkel) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from snorkel) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from snorkel) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from snorkel) (2.2.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from snorkel) (2.15.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from snorkel) (3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->snorkel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->snorkel) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->snorkel) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.2->snorkel) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.2->snorkel) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->snorkel) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->snorkel) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.2.0->snorkel) (12.4.127)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->snorkel) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->snorkel) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->snorkel) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->snorkel) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->snorkel) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->snorkel) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->snorkel) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->snorkel) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->snorkel) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.2.0->snorkel) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->snorkel) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->snorkel) (3.2.2)\n",
            "Installing collected packages: munkres, snorkel\n",
            "Successfully installed munkres-1.1.4 snorkel-0.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/KeyClass_Private/scripts\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hXrGlFKaizg",
        "outputId": "ec65a4ff-9fce-40ef-f655-0b6e75c248c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KeyClass_Private/scripts\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                   encode_datasets.py  label_data.py  run_all.py\n",
            "encode_all_datasets.sh  get_data.sh         \u001b[01;34m__pycache__\u001b[0m/   train_downstream_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_data.sh"
      ],
      "metadata": {
        "id": "h3G7l2Ubgh-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66cae3e-113c-497e-afcb-385753e77e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mkdir: cannot create directory ‘agnews/’: File exists\n",
            "\u001b[32m===Downloading AG News Data...===\u001b[m\n",
            "--2024-04-13 05:40:23--  https://drive.google.com/a/illinois.edu/uc?export=download&confirm=&id=1zszTJudS8RMgTQxURkt1w2MhswNGA6Oa\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.175.101, 142.251.175.139, 142.251.175.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.175.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1zszTJudS8RMgTQxURkt1w2MhswNGA6Oa&export=download [following]\n",
            "--2024-04-13 05:40:24--  https://drive.usercontent.google.com/download?id=1zszTJudS8RMgTQxURkt1w2MhswNGA6Oa&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11561477 (11M) [application/octet-stream]\n",
            "Saving to: ‘agnews.zip’\n",
            "\n",
            "agnews.zip          100%[===================>]  11.03M  63.5MB/s    in 0.2s    \n",
            "\n",
            "2024-04-13 05:40:31 (63.5 MB/s) - ‘agnews.zip’ saved [11561477/11561477]\n",
            "\n",
            "\u001b[32m===Unzipping AG News Data...===\u001b[m\n",
            "Archive:  agnews.zip\n",
            "replace label_names.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            " extracting: label_names.txt         \n",
            "  inflating: test.txt                \n",
            "  inflating: test_labels.txt         \n",
            "  inflating: train.txt               \n",
            "  inflating: train_labels.txt        \n",
            "mkdir: cannot create directory ‘amazon/’: File exists\n",
            "\u001b[32m===Downloading Amazon Data...===\u001b[m\n",
            "--2024-04-13 05:40:45--  https://drive.google.com/a/illinois.edu/uc?export=download&confirm=&id=1pRt5mPuuVbi-ZXD8QZzw_7DlAnEg3X15\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.175.139, 142.251.175.102, 142.251.175.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.175.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1pRt5mPuuVbi-ZXD8QZzw_7DlAnEg3X15&export=download [following]\n",
            "--2024-04-13 05:40:46--  https://drive.usercontent.google.com/download?id=1pRt5mPuuVbi-ZXD8QZzw_7DlAnEg3X15&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2427 (2.4K) [text/html]\n",
            "Saving to: ‘amazon.zip’\n",
            "\n",
            "amazon.zip          100%[===================>]   2.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-13 05:40:46 (13.7 MB/s) - ‘amazon.zip’ saved [2427/2427]\n",
            "\n",
            "\u001b[32m===Unzipping Amazon Data...===\u001b[m\n",
            "Archive:  amazon.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of amazon.zip or\n",
            "        amazon.zip.zip, and cannot find amazon.zip.ZIP, period.\n",
            "mkdir: cannot create directory ‘dbpedia/’: File exists\n",
            "\u001b[32m===Downloading DBPedia Data...===\u001b[m\n",
            "--2024-04-13 05:40:49--  https://drive.google.com/a/illinois.edu/uc?export=download&confirm=&id=1nCQQAC6XwfnyKtzWlNElMtz4s12kxfe7\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.175.139, 142.251.175.102, 142.251.175.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.175.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1nCQQAC6XwfnyKtzWlNElMtz4s12kxfe7&export=download [following]\n",
            "--2024-04-13 05:40:50--  https://drive.usercontent.google.com/download?id=1nCQQAC6XwfnyKtzWlNElMtz4s12kxfe7&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2427 (2.4K) [text/html]\n",
            "Saving to: ‘dbpedia.zip’\n",
            "\n",
            "dbpedia.zip         100%[===================>]   2.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-13 05:40:50 (13.6 MB/s) - ‘dbpedia.zip’ saved [2427/2427]\n",
            "\n",
            "\u001b[32m===Unzipping DBPedia Data...===\u001b[m\n",
            "Archive:  dbpedia.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of dbpedia.zip or\n",
            "        dbpedia.zip.zip, and cannot find dbpedia.zip.ZIP, period.\n",
            "mkdir: cannot create directory ‘imdb/’: File exists\n",
            "\u001b[32m===Downloading IMDB Data...===\u001b[m\n",
            "--2024-04-13 05:40:53--  https://drive.google.com/a/illinois.edu/uc?export=download&confirm=&id=1c8X_Ooth2fQleCVz2gCXlOd3-zzE9Mws\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.175.139, 142.251.175.102, 142.251.175.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.175.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1c8X_Ooth2fQleCVz2gCXlOd3-zzE9Mws&export=download [following]\n",
            "--2024-04-13 05:40:53--  https://drive.usercontent.google.com/download?id=1c8X_Ooth2fQleCVz2gCXlOd3-zzE9Mws&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26125114 (25M) [application/octet-stream]\n",
            "Saving to: ‘imdb.zip’\n",
            "\n",
            "imdb.zip            100%[===================>]  24.91M  70.6MB/s    in 0.4s    \n",
            "\n",
            "2024-04-13 05:40:55 (70.6 MB/s) - ‘imdb.zip’ saved [26125114/26125114]\n",
            "\n",
            "\u001b[32m===Unzipping IMDB Data...===\u001b[m\n",
            "Archive:  imdb.zip\n",
            "replace label_names.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            " extracting: label_names.txt         \n",
            "  inflating: test.txt                \n",
            "  inflating: test_labels.txt         \n",
            "  inflating: train.txt               \n",
            "  inflating: train_labels.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_all.py --config ../config_files/config_imdb.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgarSphqfCQL",
        "outputId": "f17bae4e-c09b-4e2b-ca79-6dad115ce708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Encoding Dataset\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n",
            ".gitattributes: 100% 744/744 [00:00<00:00, 3.59MB/s]\n",
            "1_Pooling/config.json: 100% 190/190 [00:00<00:00, 1.14MB/s]\n",
            "README.md: 100% 3.73k/3.73k [00:00<00:00, 22.0MB/s]\n",
            "config.json: 100% 594/594 [00:00<00:00, 3.24MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 648kB/s]\n",
            "model.safetensors: 100% 438M/438M [00:43<00:00, 10.1MB/s]\n",
            "pytorch_model.bin: 100% 438M/438M [00:00<00:00, 443MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 269kB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.47MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.86MB/s]\n",
            "tokenizer_config.json: 100% 1.19k/1.19k [00:00<00:00, 7.23MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 946kB/s]\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.35MB/s]\n",
            "Batches: 100% 196/196 [01:37<00:00,  2.01it/s]\n",
            "Batches: 100% 196/196 [01:34<00:00,  2.07it/s]\n",
            "Labeling Data\n",
            "{'activation': 'torch.nn.LeakyReLU()', 'average': 'weighted', 'base_encoder': 'paraphrase-mpnet-base-v2', 'criterion': \"torch.nn.CrossEntropyLoss(reduction='none')\", 'data_path': '/content/drive/MyDrive/KeyClass_Private/scripts/data', 'dataset': 'imdb', 'device': 'cuda', 'end_model_batch_size': 128, 'end_model_epochs': 20, 'end_model_lr': '1e-4', 'end_model_patience': 3, 'end_model_weight_decay': '1e-4', 'h_sizes': [768, 256, 64, 2], 'label_model': 'data_programming', 'label_model_lr': 0.01, 'label_model_n_epochs': 100, 'max_num': 7000, 'min_df': 0.001, 'model_path': '../models/imdb/', 'n_bootstrap': 100, 'n_classes': 2, 'n_jobs': 10, 'ngram_range': (1, 2), 'normalize_embeddings': False, 'preds_path': '../results/imdb/', 'q_update_interval': 50, 'results_path': '../results/imdb/', 'self_train_batch_size': 8, 'self_train_lr': '1e-6', 'self_train_patience': 3, 'self_train_thresh': '1-2e-3', 'self_train_weight_decay': '1e-4', 'show_progress_bar': True, 'target_0': 'negative, hate, expensive, bad, poor, broke, waste, horrible, would not recommend', 'target_1': 'good, positive, excellent, amazing, love, fine, good quality, would recommend', 'topk': 300, 'use_custom_encoder': False, 'use_noise_aware_loss': True}\n",
            "Getting labels for the imdb data...\n",
            "Size of the data: 25000\n",
            "Class distribution (array([0, 1]), array([12500, 12500]))\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n",
            "Found assigned category counts [6789 9578]\n",
            "labeler.vocabulary:\n",
            " 16367\n",
            "labeler.word_indicator_matrix.shape (25000, 600)\n",
            "Len keywords 600\n",
            "assigned_category: Unique and Counts (array([0, 1]), array([300, 300]))\n",
            "negative, hate, expensive, bad, poor, broke, waste, horrible, would not recommend ['abominable' 'abomination' 'absolute worst' 'absolutely awful'\n",
            " 'absolutely terrible' 'abuse' 'abused' 'abusive' 'abysmal'\n",
            " 'acting horrible' 'acting poor' 'acting terrible' 'actors bad'\n",
            " 'actually bad' 'also bad' 'among worst' 'annoyance' 'annoying' 'appalled'\n",
            " 'appalling' 'atrocious' 'awful' 'awfully' 'awfulness' 'bad' 'bad actor'\n",
            " 'bad actors' 'bad actually' 'bad almost' 'bad bad' 'bad could'\n",
            " 'bad either' 'bad enough' 'bad even' 'bad film' 'bad films' 'bad get'\n",
            " 'bad horror' 'bad idea' 'bad like' 'bad made' 'bad makes' 'bad many'\n",
            " 'bad movie' 'bad movies' 'bad music' 'bad one' 'bad ones' 'bad people'\n",
            " 'bad really' 'bad reviews' 'bad special' 'bad story' 'bad taste'\n",
            " 'bad thing' 'bad things' 'bad think' 'bad way' 'bad well' 'bad would'\n",
            " 'baddies' 'badly' 'badly made' 'badness' 'best worst' 'better worse'\n",
            " 'big disappointment' 'chose' 'complain' 'complained' 'complaining'\n",
            " 'complains' 'crap like' 'crappy' 'criticism' 'criticisms' 'criticize'\n",
            " 'criticized' 'cursed' 'cursing' 'cynical' 'cynicism' 'depressed'\n",
            " 'depressing' 'despair' 'desperation' 'despicable' 'despise'\n",
            " 'disappointed' 'disappointing' 'disappointment' 'disastrous' 'disdain'\n",
            " 'disgust' 'disgusted' 'disgusting' 'dislike' 'disliked' 'dismal'\n",
            " 'displeasure' 'distasteful' 'distraught' 'dreaded' 'dreadful'\n",
            " 'dreadfully' 'easily worst' 'else say' 'even bad' 'even worst'\n",
            " 'far worse' 'far worst' 'feel bad' 'feel sorry' 'film awful' 'film bad'\n",
            " 'film terrible' 'film worst' 'films bad' 'filth' 'filthy' 'get bad'\n",
            " 'god awful' 'greed' 'hapless' 'hate' 'hate film' 'hate movie' 'hated'\n",
            " 'hated movie' 'hateful' 'hates' 'hating' 'hatred' 'hideous' 'hideously'\n",
            " 'honestly say' 'horrendous' 'horrible' 'horrible film' 'horrible movie'\n",
            " 'horribly' 'horrid' 'horrific' 'huge disappointment' 'humble opinion'\n",
            " 'idiotic' 'immoral' 'incredibly bad' 'incredibly stupid' 'irresponsible'\n",
            " 'know bad' 'lack' 'lackluster' 'laughably bad' 'least good' 'like bad'\n",
            " 'like cheap' 'like horror' 'like least' 'like say' 'loathing' 'lot bad'\n",
            " 'lousy' 'love bad' 'love hate' 'low quality' 'made bad'\n",
            " 'major disappointment' 'make bad' 'many bad' 'mean spirited' 'mediocrity'\n",
            " 'miserable' 'miserably' 'misery' 'misfortune' 'misguided' 'movie awful'\n",
            " 'movie bad' 'movie horrible' 'movie terrible' 'movie worst' 'movies bad'\n",
            " 'much worse' 'nasty' 'needless say' 'needlessly' 'negative comments'\n",
            " 'negative reviews' 'never want' 'nothing good' 'notorious' 'one awful'\n",
            " 'one bad' 'one worst' 'opinion' 'opinions' 'pathetic' 'people hate'\n",
            " 'pity' 'plain awful' 'plain bad' 'plain stupid' 'poor' 'poor quality'\n",
            " 'poorest' 'poorly' 'possibly worst' 'poverty' 'pretty awful' 'pretty bad'\n",
            " 'pretty lame' 'pretty poor' 'probably worst' 'quite bad' 'rather poor'\n",
            " 'really annoying' 'really awful' 'really bad' 'really disappointed'\n",
            " 'really hate' 'really poor' 'really sad' 'really stupid'\n",
            " 'really terrible' 'refuse' 'regret' 'regrets' 'reject' 'repulsive'\n",
            " 'rotten' 'sad' 'sad thing' 'saddest' 'sadistic' 'sadly' 'sadness'\n",
            " 'say bad' 'say worst' 'see bad' 'seedy' 'simply awful' 'something bad'\n",
            " 'still bad' 'story bad' 'stupid' 'stupidest' 'stupidity' 'stupidly'\n",
            " 'sucks' 'terrible' 'terrible film' 'terrible movie' 'terribly'\n",
            " 'think bad' 'tiresome' 'trash' 'trashy' 'truly awful' 'truly bad' 'ugly'\n",
            " 'unappealing' 'unattractive' 'unbearably' 'uneducated' 'unfortunate'\n",
            " 'unfortunately' 'unfortunately film' 'unhappy' 'unimaginative'\n",
            " 'unimpressive' 'uninteresting' 'unlikable' 'unlikeable' 'unlucky'\n",
            " 'unnecessarily' 'unpleasant' 'unrealistic' 'unremarkable' 'unsatisfied'\n",
            " 'unsatisfying' 'unsympathetic' 'unwilling' 'waste money' 'worse'\n",
            " 'worse movie' 'worse movies' 'worst' 'worst acting' 'worst ever'\n",
            " 'worst film' 'worst films' 'worst kind' 'worst movie' 'worst movies'\n",
            " 'worst part' 'worst thing' 'worthless' 'wretched' 'writing bad']\n",
            "good, positive, excellent, amazing, love, fine, good quality, would recommend ['actually good' 'actually like' 'admirable' 'admirably' 'admired'\n",
            " 'almost good' 'also enjoyed' 'also excellent' 'also good' 'also great'\n",
            " 'also interesting' 'also like' 'also liked' 'also nice' 'also pretty'\n",
            " 'also well' 'always good' 'always great' 'among best' 'another good'\n",
            " 'another great' 'another reviewer' 'anyone likes' 'anything good'\n",
            " 'award best' 'bad good' 'best' 'best best' 'best ever' 'best one'\n",
            " 'best parts' 'best performance' 'best show' 'best thing' 'best things'\n",
            " 'better ones' 'certainly good' 'commendable' 'damn good'\n",
            " 'definitely good' 'definitely recommend' 'definitely worth' 'done good'\n",
            " 'done great' 'enjoy' 'enjoy good' 'enjoyable' 'enjoyable film'\n",
            " 'enjoyable movie' 'enjoyable watch' 'enough good' 'entertainment value'\n",
            " 'especially good' 'especially like' 'especially liked' 'even good'\n",
            " 'even great' 'excellence' 'excellent' 'excellent film' 'excellent job'\n",
            " 'excellent movie' 'excellent performance' 'excellent performances'\n",
            " 'excellently' 'exquisite' 'extremely well' 'fabulous' 'fairly good'\n",
            " 'fantastic' 'far best' 'film excellent' 'find good' 'fine job'\n",
            " 'fine performances' 'finest' 'first rate' 'get good' 'give good'\n",
            " 'gives best' 'gives good' 'gives great' 'good' 'good action' 'good also'\n",
            " 'good although' 'good bad' 'good choice' 'good direction' 'good either'\n",
            " 'good enough' 'good entertainment' 'good especially' 'good even'\n",
            " 'good example' 'good film' 'good films' 'good first' 'good good'\n",
            " 'good great' 'good idea' 'good movie' 'good music' 'good one' 'good ones'\n",
            " 'good original' 'good part' 'good parts' 'good people' 'good performance'\n",
            " 'good performances' 'good really' 'good reviews' 'good say' 'good show'\n",
            " 'good special' 'good stuff' 'good taste' 'good thing' 'good things'\n",
            " 'good think' 'good though' 'good tv' 'good use' 'good well' 'good work'\n",
            " 'good would' 'good writing' 'got good' 'got great' 'great'\n",
            " 'great character' 'great example' 'great film' 'great fun' 'great love'\n",
            " 'great music' 'great one' 'great really' 'great show' 'great supporting'\n",
            " 'great things' 'great time' 'greats' 'high quality' 'high rating'\n",
            " 'highly recommend' 'highly recommended' 'however like' 'idea good'\n",
            " 'like best' 'like good' 'like great' 'liked' 'liked one' 'looks great'\n",
            " 'lot good' 'lot great' 'love good' 'lovely' 'luxury' 'made good'\n",
            " 'made great' 'made well' 'make good' 'make great' 'makes good'\n",
            " 'makes great' 'many good' 'many great' 'many reviewers' 'many reviews'\n",
            " 'marvelously' 'may good' 'mean good' 'might good' 'movie excellent'\n",
            " 'movie good' 'movie recommend' 'movie wonderful' 'much enjoyed'\n",
            " 'much good' 'music good' 'music great' 'nearly good' 'nice' 'nice look'\n",
            " 'nothing better' 'one best' 'one finest' 'one good' 'one great'\n",
            " 'one like' 'overall good' 'overall think' 'particularly good'\n",
            " 'people good' 'people like' 'performances good' 'perhaps best'\n",
            " 'personal favorite' 'personally think' 'pleasant' 'positive reviews'\n",
            " 'positive thing' 'possibly best' 'praise' 'prefer' 'prefers'\n",
            " 'pretty decent' 'pretty good' 'probably best' 'probably good'\n",
            " 'probably like' 'put good' 'qualities' 'quality' 'quality acting'\n",
            " 'quite enjoyable' 'quite good' 'quite like' 'rather good' 'rating 10'\n",
            " 'read review' 'read reviews' 'reading reviews' 'real good'\n",
            " 'really appreciate' 'really enjoy' 'really enjoyed' 'really good'\n",
            " 'really great' 'really like' 'really liked' 'really loved' 'really nice'\n",
            " 'really recommend' 'recommend' 'recommend anyone' 'recommend everyone'\n",
            " 'recommend film' 'recommend movie' 'recommend one' 'recommend see'\n",
            " 'recommend watch' 'recommend watching' 'recommendation' 'recommended'\n",
            " 'recommending' 'redeeming quality' 'reviews' 'satisfactory' 'say best'\n",
            " 'say good' 'see good' 'seen good' 'show good' 'solid performances'\n",
            " 'something better' 'something good' 'something interesting' 'splendid'\n",
            " 'still enjoyable' 'still good' 'still great' 'strongly recommend'\n",
            " 'surprisingly good' 'tasteful' 'terrific' 'thing good' 'think best'\n",
            " 'think good' 'think great' 'though good' 'thought good' 'thought great'\n",
            " 'time great' 'top notch' 'truly great' 'two best' 'want good'\n",
            " 'watch good' 'well crafted' 'well good' 'well great' 'well made'\n",
            " 'well produced' 'well worth' 'wonderful' 'wonderful film' 'wonderful job'\n",
            " 'wonderful life' 'wonderful movie' 'wonderfully' 'worth look'\n",
            " 'worth mentioning' 'worth seeing' 'worthwhile' 'would good'\n",
            " 'would recommend']\n",
            "==== Training the label model ====\n",
            "INFO:root:Computing O...\n",
            "INFO:root:Estimating \\mu...\n",
            "INFO:root:Using GPU...\n",
            "  0% 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.191]\n",
            "100% 100/100 [00:03<00:00, 30.20epoch/s]\n",
            "INFO:root:Finished Training\n",
            "Label Model Predictions: Unique value and counts (array([0, 1]), array([ 8914, 16086]))\n",
            "Label Model Training Accuracy 0.70016\n",
            "Saving results in ../results/imdb/train_label_model_with_ground_truth_13-Apr-2024-05_46_13.txt...\n",
            "Training Model\n",
            "Confidence of least confident data point of class 0: 0.9118952422355573\n",
            "Confidence of least confident data point of class 1: 0.999915738903688\n",
            "\n",
            "==== Data statistics ====\n",
            "Size of training data: (25000, 768), testing data: (25000, 768)\n",
            "Size of testing labels: (25000,)\n",
            "Size of training labels: (25000,)\n",
            "Training class distribution (ground truth): [0.5 0.5]\n",
            "Training class distribution (label model predictions): [0.35656 0.64344]\n",
            "\n",
            "KeyClass only trains on the most confidently labeled data points! Applying mask...\n",
            "\n",
            "==== Data statistics (after applying mask) ====\n",
            "Size of training data: (7000, 768)\n",
            "Size of training labels: (7000,)\n",
            "Training class distribution (ground truth): [0.55057143 0.44942857]\n",
            "Training class distribution (label model predictions): [0.5 0.5]\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n",
            "\n",
            "===== Training the downstream classifier =====\n",
            "\n",
            "Epoch 17:  85% 17/20 [00:03<00:00,  4.98batch/s, best_loss=0.543, running_loss=0.547, tolerance_count=3]Stopping early...\n",
            "Epoch 17:  85% 17/20 [00:03<00:00,  4.55batch/s, best_loss=0.543, running_loss=0.547, tolerance_count=3]\n",
            "Saving model end_model_13-Apr-2024-05_46_17.pth...\n",
            "Saving results in ../results/imdb/train_end_model_with_ground_truth_13-Apr-2024-05_46_20.txt...\n",
            "Saving results in ../results/imdb/train_end_model_with_label_model_13-Apr-2024-05_46_20.txt...\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    3.2s finished\n",
            "Saving results in ../results/imdb/test_end_model_with_ground_truth_13-Apr-2024-05_46_24.txt...\n",
            "\n",
            "===== Self-training the downstream classifier =====\n",
            "\n",
            "Epoch 1:   2% 1/62 [02:05<2:07:10, 125.09s/batch, self_train_agreement=0.985, tolerance_count=0, validation_accuracy=0.853]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBCRs_srdKw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}